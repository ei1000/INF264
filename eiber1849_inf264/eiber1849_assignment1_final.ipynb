{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c12c05b4-ecf9-4f2f-9516-0b37d19e73ff",
   "metadata": {},
   "source": [
    "### Eiber1849 obligatory assignment 1\n",
    "# Make a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce77f734-cae6-4ec2-aace-df325a9cd059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Important modules\n",
    "\n",
    "#Modules for the implementation\n",
    "import numpy as np\n",
    "import time\n",
    "from math import log2\n",
    "\n",
    "#Modules for testing\n",
    "import csv\n",
    "from sklearn import neighbors, datasets, model_selection, metrics, __version__\n",
    "from sklearn import tree as sk_tree\n",
    "import pandas as pd\n",
    "\n",
    "#Incase wrong working directory\n",
    "#import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b3c957-8259-42d2-9d48-b2979142b52a",
   "metadata": {},
   "source": [
    "# Creating the the different functions.\n",
    "\n",
    "## Structure of the implementation\n",
    "\n",
    "* learn(X, y, impurity_measure='entropy', pruning=False, train_ratio=0.8)\n",
    "  >Takes in training data, labels and tree object. Can choose impurity_measure with default='entropy' or 'gini'. Can also enable pruning, which will split the input data into training and pruning. Returns a trained tree\n",
    "* predict(data_point, tree)\n",
    "  >Takes a datapoint and does a search on the tree. Returns predicted label\n",
    "\n",
    "These are the main functions. Since I am implementing this functionally, the order of functions will be bottom-up. I.e The main function for last. \n",
    "\n",
    "#### The learn function's main sub-functions are;\n",
    "\n",
    "* id3(data, labels, tree, impurity_measure = 'entropy')\n",
    "  >Takes data, labels, tree and modifies the tree. Void function, but builds on tree object\n",
    "* prune(data, labels, tree, impurity_measure)\n",
    "  >Takes data, labels, tree and modifies the tree. Checking if subtrees are necessary.\n",
    "\n",
    "#### id3 and prune use the same smaller help functions;\n",
    "\n",
    "**figuring out how to split**\n",
    "1. find_best_feature(data, labels, impurity_measure = 'entropy')\n",
    "   >takes data and labels, then returns the best feature and value\n",
    "2. investigation_score(feature, labels, split_point, impurity_measure='entropy')\n",
    "   >returns a score on how good it is to split on the feature \n",
    "3. cond_impurity(feature, labels, split_point, impurity_measure='entropy')\n",
    "   >returns impurity of splitting on a feature\n",
    "4. impurity(labels_arr, impurity_measure = 'entropy')\n",
    "   >returns a value on how pure the data is. 0 is pure, higher is more unpure.\n",
    "\n",
    "**Splitting data**\n",
    "1. split_data(data, labels, split_feature, split_point)\n",
    "   >Takes data, labels, where to split and on what value. Returns left(data, labels), right(data, labels) and information on how the split was done\n",
    "2. split_label(feature, labels, split_point)\n",
    "   >Returns left(labels) and right(labels). Splits labels based on indexes where feature values are lower or higher than split_point\n",
    "\n",
    "### starting with the tree class\n",
    "Binary tree structure with left and right child. The value is the differantiatior between root and leaf. Root nodes have tuples, while leaf node has integers. Could rather have implemented self.label, self.split_point, self.feature_split. This is more compact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f606655-3fed-4eec-98a4-28c817bb71a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple tree structure with maximum 2 children\n",
    "class Tree:\n",
    "    \n",
    "    #self.value is eihter a tuple or an int. If it is a tuple, then it is a root with (feature, split_value). If it is an int, then it a lable. \n",
    "    def __init__(self):\n",
    "        #Value for split and majority label\n",
    "        self.value = None\n",
    "        self.major_label = None\n",
    "\n",
    "        #Children of the tree\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceb588e-5882-490a-8568-964ed07423cf",
   "metadata": {},
   "source": [
    "### Split labels\n",
    "split labels on a value, split_point. Could sort the array and split on given value with np.split. But that adds sorting complexity, so using np.nonzero to get indexes of datapoints that satisfies requirement. Then index them out of labels and return the two arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c9899cf-0a7d-4c27-b616-38dbd1b161e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take feature, lable and split_point. Return two splitted arrays \n",
    "def split_label(feature, labels, split_point):\n",
    "    #Splitting the feature on the split_point\n",
    "    left_labels = labels[np.nonzero(feature < split_point)]\n",
    "    right_labels = labels[np.nonzero(feature >= split_point)]\n",
    "    \n",
    "    #np.nonzero function finds the index in an array with a condition. Here the conditions are based on the mean.\n",
    "    return left_labels, right_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d8f850-8419-4bea-8ad3-845492dfd053",
   "metadata": {},
   "source": [
    "### Impurity and investigation score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb28590-5ec0-4973-9c96-e811daa9abb3",
   "metadata": {},
   "source": [
    "##### Vectorized or for loop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "614615a6-292c-420e-adac-adec7d35c7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take an array of labels, then return a score to its purity. Either with entropy or gini\n",
    "def impurity(labels_arr, impurity_measure = 'entropy'):\n",
    "    \n",
    "    #Finding the counts for each possible label. Eihter one or two labels. \n",
    "    labels, counts = np.unique(labels_arr, return_counts=True)\n",
    "    \n",
    "    #Making a temp function to find possibility. Not necessary but the code is more readable later.  \n",
    "    prob = lambda x: x/len(labels_arr)\n",
    "\n",
    "    #If the measure is entropy use the entropy formula. And return entropy\n",
    "    if impurity_measure == 'entropy':\n",
    "        entropy = 0\n",
    "        for count in counts:\n",
    "            entropy -= prob(count)*log2(prob(count))\n",
    "        return entropy\n",
    "\n",
    "    #If measure is gini, use gini formula and return gini\n",
    "    #Gini should be faster to compute, since it uses substraction instead of log2\n",
    "    elif impurity_measure == 'gini':\n",
    "        gini = 0\n",
    "        for count in counts:\n",
    "            gini += prob(count)*(1-prob(count))\n",
    "        return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8e3ab8e-9f29-444c-b215-40a6723dc519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as above\n",
    "def impurity_vec(labels_arr, impurity_measure = 'entropy'):\n",
    "    #Finding the counts for each possible label. Eihter one or two labels. \n",
    "    labels, counts = np.unique(labels_arr, return_counts=True)\n",
    "    \n",
    "    #Using temp function that implements the entropy formula, -sum(prob*log2(prob)), then vectorizes the counts array so we can take the sum to calculate entropy.\n",
    "    #Has to return the negative of the sum, since log2 of decimal is negative.\n",
    "    if impurity_measure == 'entropy':\n",
    "        entropy = lambda x: x/len(labels_arr)*log2(x/len(labels_arr))\n",
    "        return -np.vectorize(entropy)(counts).sum()\n",
    "\n",
    "    #Doing the same but with gini formula. sum(prob*(1-prob)\n",
    "    elif impurity_measure == 'gini':\n",
    "        gini = lambda y: y/len(labels_arr)*(1-y/len(labels_arr))\n",
    "        return np.vectorize(gini)(counts).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c9f597e-87bf-4570-b04e-062c05dae958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing vectorized is -0.012667222023010254 seconds faster per impurity computation. If it is negative, then it is slower\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9182958340544896, 0.9182958340544896)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run this to see which is faster\n",
    "def impurity_speed(impurity_option=impurity, measure='entropy'):\n",
    "    start_time = time.time()\n",
    "    for x in range(0,500):\n",
    "        impurity_option([1,0], measure)\n",
    "    end_time = time.time()\n",
    "    return end_time-start_time\n",
    "\n",
    "speed_test = np.array([[impurity_speed(impurity, 'entorpy') - impurity_speed(impurity_vec, 'gini')] for x in range(0,100)])\n",
    "\n",
    "print(f'Doing vectorized is {speed_test.mean()} seconds faster per impurity computation. If it is negative, then it is slower')\n",
    "\n",
    "impurity([1,1,0]), impurity_vec([1,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c44a50-cccd-41d2-9409-abb1a24b7ff5",
   "metadata": {},
   "source": [
    "Running code above shows that it is more effective to not vectorize and just use for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f2b1d26-d624-4935-88c2-538d59b71ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take an array of labels, then split it based on given feature and split point. Then return the impurity if the labels are split.\n",
    "def cond_impurity(feature, labels, split_point, impurity_measure='entropy'):\n",
    "    #Finding mean\n",
    "    \n",
    "    #Getting the labels for the split\n",
    "    left_labels, right_labels = split_label(feature, labels, split_point=split_point)\n",
    "    \n",
    "    #Finding the count of labels on each side\n",
    "    left_len = len(left_labels)/len(feature)\n",
    "    right_len = len(right_labels)/len(feature)\n",
    "\n",
    "    #Calculating the impurity measure for the feature and each split, and multiplying with the split ratio\n",
    "    impurity_left = impurity(left_labels, impurity_measure)*left_len\n",
    "    impurity_right = impurity(right_labels, impurity_measure)*right_len\n",
    "\n",
    "    #returning the conditional impurity\n",
    "    return impurity_left + impurity_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1194033f-f4f4-4e0b-99da-b8e0ea693c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take an array of labels, check how good it is to split it given feature and split_point. Return the score.\n",
    "def investigation_score(feature, labels, split_point, impurity_measure='entropy'):\n",
    "\n",
    "    #Getting the impurity\n",
    "    impurity_feature = impurity(labels, impurity_measure)\n",
    "    cond_impurity_feature = cond_impurity(feature, labels, split_point, impurity_measure)\n",
    "\n",
    "    #Returning the investigation score\n",
    "    return impurity_feature - cond_impurity_feature\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953ee1c9-b929-455d-a2d6-74224314091f",
   "metadata": {},
   "source": [
    "### Finding the best feature to split on, by investigation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c8a4a07-63fd-408f-9237-df50026d30f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the best feature given data and labels. Check each feature for its investigation score, then return the best feature, and split\n",
    "def find_best_feature(data, labels, impurity_measure = 'entropy'):\n",
    "    #Making a dictionary to store the feature key and the inv_score as value. \n",
    "    best_i = {}\n",
    "    #Iterating through each feature\n",
    "    for feature_index in range(data.shape[1]):\n",
    "        feature = data[:, feature_index]\n",
    "\n",
    "        #Making a tuple for each feature with index and value\n",
    "        best_i[feature_index] = investigation_score(data[:, feature_index], labels, feature.mean(), impurity_measure=impurity_measure), feature.mean()\n",
    "\n",
    "    #Finding best feature with max function, where the key is the values. \n",
    "    best_feature_index = max(best_i, key= lambda x: best_i[x][0])\n",
    "\n",
    "    return best_feature_index, best_i[best_feature_index][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef99aa0-f964-4807-bf66-c9368154eeec",
   "metadata": {},
   "source": [
    "### Splitting the data and labels into subarrays\n",
    "Split the data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b5c15c8-1225-4c78-8f2b-4d6502133dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data and labels based on a given feature and value. Return the splitted data, labels and information about the split.\n",
    "def split_data(data, labels, split_feature, split_point):\n",
    "    feature = data[:, split_feature]\n",
    "\n",
    "    #Splitting the labels | Could have returned the labels in investigation_score to save compute, but this should be marginal.\n",
    "    left_labels, right_labels = split_label(feature, labels, split_point)  \n",
    "    \n",
    "    #Splitting the data based on the indexes of what points is lower or higher than mean, given specific feature\n",
    "    \n",
    "    #print(f'split feature: {feature__}')\n",
    "    \n",
    "    left_data = data[feature < split_point]\n",
    "    right_data = data[feature >= split_point]\n",
    "    \n",
    "    #Returning a 3-tuple consisting of the left side, the right side and information about the split (what feature, the mean)\n",
    "    #The latter will be stored in each branch when building the tree. \n",
    "    return (left_data, left_labels), (right_data, right_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0cae93-4c02-41a4-86ec-b6b2629c5685",
   "metadata": {},
   "source": [
    "### Checking for identical features\n",
    "If there is only one unique value in every column, then the multi array has identical rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68c5f8f2-a3ee-4cb8-b950-e61c9a892610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identical_features(data, labels):\n",
    "    count = 0\n",
    "    for feature in range(data.shape[1]):\n",
    "        if len(np.unique(data[:,feature])) == 1:\n",
    "            count += 1\n",
    "    \n",
    "    if count == data.shape[1]:\n",
    "        uniques, counts = np.unique(labels, return_counts=True)\n",
    "        return uniques[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411f482b-d36e-4c45-af7e-f9be017dabd4",
   "metadata": {},
   "source": [
    "## Implementing the ID3 function by using the prior built functions\n",
    "\n",
    "The algorithm takes in data, labels and a tree. It  has the feature impurity_measure, which decides how impurity is computed. It builds on the input tree by creating new subtrees. The function does not return the tree, but modifies it from top-down. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61eb97a8-1a75-45bc-a0f3-3ed7c568316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take data and labels, then build on given tree by splitting or setting lables.\n",
    "def id3(data, labels, tree, impurity_measure = 'entropy'):\n",
    "    #Finding checking for identical features\n",
    "    identical = identical_features(data, labels)\n",
    "    \n",
    "    #If all data points have the same label:\n",
    "    if impurity(labels) == 0:\n",
    "        tree.value = labels[0]\n",
    "        return labels[0]\n",
    "\n",
    "    #Else if all data points have identical feature values\n",
    "    elif identical != None:\n",
    "        tree.value = identical\n",
    "        return\n",
    "\n",
    "\n",
    "    #Else\n",
    "    else:\n",
    "        #Finding the best feature to split on\n",
    "        split_feature, split_point = find_best_feature(data, labels, impurity_measure = impurity_measure)\n",
    "        \n",
    "        #Extracting the information from the split\n",
    "        left, right = split_data(data, labels, split_feature, split_point)\n",
    "        \n",
    "        #Setting this root to indicate the split\n",
    "        tree.value = split_feature, split_point\n",
    "\n",
    "        #Setting the majority label\n",
    "        lab, counts = np.unique(labels, return_counts=True)\n",
    "        maj_index = np.where(counts == max(counts))[0][0]\n",
    "        tree.majority_label = lab[maj_index]\n",
    "    \n",
    "        #Making left branch\n",
    "        new_left = Tree()\n",
    "        tree.left = new_left\n",
    "        id3(left[0], left[1], new_left, impurity_measure)\n",
    "    \n",
    "        #Making right branch\n",
    "        new_right = Tree()\n",
    "        tree.right = new_right \n",
    "        id3(right[0], right[1], new_right, impurity_measure)\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb721d-f322-4d49-88ee-0907d4dc661e",
   "metadata": {},
   "source": [
    "## Creating some functions to inspect the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b2b94a1-0c6a-44d6-b6f0-ed31e092fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a tree and prints the value for each node\n",
    "def search_tree(tree):\n",
    "    #As long as it is a tree object\n",
    "    if tree != None:\n",
    "        print(tree.value)\n",
    "        \n",
    "        #Go left and right\n",
    "        search_tree(tree.left)\n",
    "        search_tree(tree.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8408223a-20e0-4cf5-97c8-adbe5838010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns total amount of nodes in a tree.\n",
    "def total_nodes(tree):\n",
    "    #If it is not a tree, then return 0 and go up\n",
    "    if tree == None:\n",
    "        return 0\n",
    "\n",
    "    #Go left most and then right\n",
    "    l = total_nodes(tree.left)\n",
    "    r = total_nodes(tree.right)\n",
    "\n",
    "    #Return 1 and then the value of the left and right children.\n",
    "    return 1 + l + r\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059cb26c-d81d-4a74-a66b-10c7a1935090",
   "metadata": {},
   "source": [
    "## Prediction and accuracy functions\n",
    "\n",
    "Binary tree properties, searching the tree will be in O(h) time, where h is the height. This makes the pruning efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31585334-147f-4159-a803-00a0041c0775",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a datapoint and a tree, then returns a label based on the tree\n",
    "def predict(data_point, tree):\n",
    "    #Only the leaf node is not a tuple. So go through all the tuples and go leaft or right until you reach a leaf\n",
    "    while type(tree.value) == tuple: #and tree.value != None | is unecesarry since every split has a child or is a leaf. So it wont trigger non leafs or splits\n",
    "        feature, split_point = tree.value\n",
    "        \n",
    "        #Go left or right based on split_point\n",
    "        if data_point[feature] < split_point:\n",
    "            tree = tree.left\n",
    "        else:\n",
    "            tree = tree.right\n",
    "\n",
    "    #When you reach a leaf, return the value/label\n",
    "    else:\n",
    "        return tree.value\n",
    "    #print(tree.value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5516f7fe-07d3-442b-bd31-568743f2cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes in data, labels and tree and returns the trees accuracy based on the data and labels. \n",
    "def tree_accuracy(data, true_labels, tree):\n",
    "    #Make an array of all the predictions for the data_points, with given tree, then return the ratio of correctly labled predictions. \n",
    "    predictions = np.array([predict(data_point, tree) for data_point in data])\n",
    "    \n",
    "    #Return accuracy\n",
    "    return len(predictions[predictions == true_labels])/len(true_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "620bb807-8adc-44bf-b06e-422ce1705769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes an array of labels, and a prediction label. Then returns the accuracy of labels that match the prediction label.\n",
    "def majority_accuracy(labels, prediction):\n",
    "    return len(labels[labels == prediction])/len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780f19f9-3451-404c-b881-af3079449e6b",
   "metadata": {},
   "source": [
    "## Pruning algorithm\n",
    "A Depth first search. Tree structure has a left and right child. Recursively move down by going as much left as possible, then right. On each subtree calculate the accuracy, then compare to the accuracy if it is a leaf with majority label. If it is better or the same, change it to a leaf.\n",
    "\n",
    "If the pruning labels are empty on a split further down, then it cannot make a certain decision. So it will just go up a level. Replacing the subtree in such condition can make the model underfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe8949d9-f022-4fa1-be44-71f2b4cfd408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a tree, and change it based on some data and labels\n",
    "def prune(data, labels, tree):\n",
    "    #Just want to do the pruning on a node that is splitting, not on a label node.\n",
    "    if type(tree.value) == tuple:\n",
    "        feature, mean = tree.value\n",
    "\n",
    "        #If the splitted labels are empty, then there is no reason to continue down\n",
    "        #Leave it as subtree, because it is uncertain if a split or leaf is optimal.\n",
    "        if len(labels) == 0:\n",
    "            return\n",
    "\n",
    "        #Else continue with the search\n",
    "        else:\n",
    "            left, right = split_data(data, labels, feature, mean)\n",
    "\n",
    "        \n",
    "        #If there is a left child go left\n",
    "        if type(tree.left) == Tree:\n",
    "            prune(left[0], left[1], tree.left)\n",
    "            \n",
    "        #If there is a right child go right\n",
    "        if type(tree.right) == Tree:\n",
    "            prune(right[0], right[1], tree.right)\n",
    "\n",
    "        #Accuracy of either splitting or majority label from training \n",
    "        #If the accuracy of majority class is greater than the prediction of subtree, then replace the subtree with the majority lable. \n",
    "        if majority_accuracy(labels, tree.majority_label) >= tree_accuracy(data, labels, tree):\n",
    "            #Replacing subtree with label\n",
    "            tree.value = tree.majority_label\n",
    "            tree.left, tree.right = None, None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8765ca3-3a21-49f1-a2f0-68a3e7583f37",
   "metadata": {},
   "source": [
    "# The main learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46abd623-47e2-4eac-9b2d-84f6b9df45c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the id3 algorithm to return a decision tree, and perhaps alter it with pruning if turned on.\n",
    "def learn(X, y, impurity_measure='entropy', pruning=False, train_ratio=0.8):\n",
    "    #Making the root for the tree\n",
    "    tree = Tree()\n",
    "\n",
    "    #Checking whether the pruning is true\n",
    "    if pruning == True:\n",
    "        #Now we need to split the data\n",
    "        train_ratio = train_ratio\n",
    "        X_train, X_prune = np.split(X, [int(train_ratio*len(X))])\n",
    "        y_train, y_prune = np.split(y, [int(train_ratio*len(y))])\n",
    "\n",
    "        #Making the tree with training data\n",
    "        id3(X_train, y_train, tree, impurity_measure)\n",
    "\n",
    "        #Pruning the tree with the pruning data\n",
    "        prune(X_prune, y_prune, tree)\n",
    "\n",
    "        #Returning the pruned tree\n",
    "        return tree\n",
    "    \n",
    "    #Else if pruning is false | Just make the tree and return it.\n",
    "    else:\n",
    "        id3(X, y, tree, impurity_measure)\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a41d14-ea49-40e8-a4b9-c1a5305851b2",
   "metadata": {},
   "source": [
    "### read csv to numpy\n",
    "Could also just use pandas and then to_numpy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9a9b874-9cc6-4e6e-b242-72ecbf8b54f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes cvs file and splits to data and targets/labels\n",
    "def cvs_numpy(name=''):\n",
    "    with open(name, 'r') as r:\n",
    "        reader = csv.reader(r)\n",
    "        data = list(reader)\n",
    "    \n",
    "    feature_names = data[0][:-1]\n",
    "    data_ar = np.array(data[1:], dtype=float)\n",
    "    targets = data_ar[:, -1]\n",
    "    data = data_ar[:, :-1]\n",
    "    return data, targets, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2522610c-45c7-4a14-9db2-fa72d2606ae9",
   "metadata": {},
   "source": [
    "# Testing and sanity checks with the wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48f03310-43a1-4880-88d5-1a6ce72b65e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in the wine dataset, and seperating data and labels\n",
    "data, labels, target_names = cvs_numpy('wine_dataset.csv')\n",
    "\n",
    "#Setting a fixed seed for reproducibility, tested with 332, 333, 521\n",
    "seed = 521\n",
    "\n",
    "#Splitting into training and val_test\n",
    "X_train, X_val_test, y_train, y_val_test = model_selection.train_test_split(data, labels, test_size=0.2, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ce9c04f-32e7-4f01-a07e-812d6967a4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.2711043357849121, accuracy: 1.0, total nodes: 873\n"
     ]
    }
   ],
   "source": [
    "#Testing runtime and training acc for non pruning. Sanity check: training acc should be 1.0\n",
    "start = time.time()\n",
    "tree_1= learn(X_train, y_train, impurity_measure='entropy')\n",
    "accu = tree_accuracy(X_train, y_train, tree_1)\n",
    "tot_nodes = total_nodes(tree_1)\n",
    "ending = time.time()\n",
    "print(f'Time: {ending-start}, accuracy: {accu}, total nodes: {tot_nodes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b7dd259-0c09-495b-9dab-a54ce7a6467b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.19340777397155762, accuracy: 0.9159499609069586, total nodes: 219\n"
     ]
    }
   ],
   "source": [
    "#Testing runtime for pruning. Sanity check: Should have lower training acc than without, and fewer nodes.\n",
    "start = time.time()\n",
    "tree_2 = learn(X_train, y_train, impurity_measure='entropy', pruning=True, train_ratio=0.7)\n",
    "accu = tree_accuracy(X_train, y_train, tree_2)\n",
    "tot_nodes = total_nodes(tree_2)\n",
    "ending = time.time()\n",
    "print(f'Time: {ending-start}, accuracy: {accu}, total nodes: {tot_nodes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae239a5-5906-4f17-b242-9ecb38387011",
   "metadata": {},
   "source": [
    "It is taking shorter with pruning because it is training with less training data. Sanity checks looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49d65284-b147-46eb-899f-5a0eea945b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with val_test data. Tree1: 0.87344 | Tree2: 0.90000\n"
     ]
    }
   ],
   "source": [
    "val_acc_1 = tree_accuracy(X_val_test, y_val_test, tree_1)\n",
    "val_acc_2 = tree_accuracy(X_val_test, y_val_test, tree_2)\n",
    "\n",
    "print(f'Testing with val_test data. Tree1: {val_acc_1:.5f} | Tree2: {val_acc_2:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7068e67a-cc40-4501-9c9d-d194c5b3bc9a",
   "metadata": {},
   "source": [
    "Looks like pruning is more general\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2be50c-943d-422d-8389-4cde572ae4ab",
   "metadata": {},
   "source": [
    "## Evaluating best modfel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d67518-ae73-46d8-acd9-eac962fb19dd",
   "metadata": {},
   "source": [
    "Assess the performance of your algorithm using an appropriate performance\r\n",
    "measure. Which setting should you select for this data (entropy or Gini,\r\n",
    "pruning or no pruning)? What is your estimate for the performance of\r\n",
    "the selected model on unseen data points? Report how you arrived at the\r\n",
    "conclusi\n",
    "\n",
    "ons.\r\n",
    "Remember to use training, validation, and test sets properly. Note that in the\r\n",
    "model selection step you select one out of the four models (settings) based\r\n",
    "on performance on validation data, and in the model evaluation step you\r\n",
    "evaluate the selected model on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b527825-eead-4a04-bacf-89b8819af9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test = np.split(X_val_test, [int(len(X_val_test)/2)])\n",
    "y_val, y_test = np.split(y_val_test, [int(len(y_val_test)/2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ce29ebd-f392-4882-97fa-b2267a2b79ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to find the best training ratio for pruning. Can also find if no pruning is better\n",
    "def best_ratio(train_data, train_labels, val_data, val_labels, impurity_measure):\n",
    "    tree_prunefree = Tree()\n",
    "    tree_prunefree = learn(train_data, train_labels, impurity_measure=impurity_measure)\n",
    "    accuracy_prunefree = tree_accuracy(val_data, val_labels, tree_prunefree)\n",
    "    #print(f'Prune free accuracy: {accuracy_prunefree}')\n",
    "    ratios = {}\n",
    "    \n",
    "    for x in range(1,10):\n",
    "        tree = Tree()\n",
    "        tree = learn(train_data, train_labels, impurity_measure=impurity_measure, pruning=True, train_ratio=x/10)\n",
    "        accuracy_tree = tree_accuracy(val_data, val_labels, tree)\n",
    "        ratios[x/10] = accuracy_tree\n",
    "        #print(f'Testing training ratio: {x/10}, with impurity_measure: gini, accuracy: {accuracy_tree:.3f} | Difference in accuracy: {accuracy_tree- accuracy_prunefree:.5f}')\n",
    "\n",
    "    #Adding no pruning to the ratios with 1 as key\n",
    "    ratios[1] = accuracy_prunefree\n",
    "\n",
    "    #Finding the best ratio based on the values with .get\n",
    "    best_ratio = max(ratios, key=ratios.get)\n",
    "\n",
    "    #In the case that no pruning is best\n",
    "    pruning = False\n",
    "\n",
    "    #If pruning is better\n",
    "    if best_ratio != 1:\n",
    "        pruning = True\n",
    "\n",
    "    #Printing some information\n",
    "    print(f'Impurity_measure: {impurity_measure}\\nAccuracy without pruning: {accuracy_prunefree:.5f}\\\n",
    "            \\nAccuracy with best pruning: {ratios[best_ratio]:.5f}, training ratio: {best_ratio}\\n')\n",
    "\n",
    "    #Returning the best model given impurity measure\n",
    "    return [best_ratio, ratios[best_ratio], impurity_measure, pruning]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbf3d8f3-cffe-46b6-9d8c-a2c9bc6582f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impurity_measure: entropy\n",
      "Accuracy without pruning: 0.89062            \n",
      "Accuracy with best pruning: 0.91250, training ratio: 0.7\n",
      "\n",
      "Impurity_measure: gini\n",
      "Accuracy without pruning: 0.88125            \n",
      "Accuracy with best pruning: 0.91875, training ratio: 0.8\n",
      "\n",
      "The best model is gini, with pruning: True, training ratio: 0.8, validation accuracy: 0.91875 \n"
     ]
    }
   ],
   "source": [
    "best_entropy = best_ratio(X_train, y_train, X_val, y_val, impurity_measure='entropy')\n",
    "best_gini = best_ratio(X_train, y_train, X_val, y_val, impurity_measure= 'gini')\n",
    "\n",
    "models = [best_entropy, best_gini]\n",
    "\n",
    "best_model = max(models, key=lambda y: y[1])\n",
    "\n",
    "training_ratio, accuracy, impurity_measure, pruning = best_model \n",
    "print(f'The best model is {impurity_measure}, with pruning: {pruning}, training ratio: {training_ratio}, validation accuracy: {accuracy:.5f} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6df1fa-9304-49c3-adfd-63bd0c51b7bc",
   "metadata": {},
   "source": [
    "With seed=521. The best performing model on the validation data is using gini and pruning with data split into 80% training and 20% pruning.\n",
    "\n",
    "Sanity check: Looks like the different impurity measures gives different accuracy. That is good.\n",
    "\n",
    "Now lets test accuracy with the test data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d9b1112-66d4-45e0-b9f7-d1f1cabc2412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model has 0.896875\n"
     ]
    }
   ],
   "source": [
    "best_model = learn(X_train, y_train, impurity_measure=impurity_measure, pruning=pruning, train_ratio=training_ratio)\n",
    "\n",
    "test_accuracy = tree_accuracy(X_test, y_test, best_model)\n",
    "\n",
    "print(f'The best model has {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e67b56-124f-4a08-aa8b-bfaa7f6f1376",
   "metadata": {},
   "source": [
    "### Conclusion on tests\n",
    "\n",
    "gini seems to be more accurate with pruning, but entropy is more accurate without pruning\n",
    "\n",
    "gini with pruning is overall the most accurate on this test data and seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5fdb0d-6be6-43de-8f4d-94fe689cea0b",
   "metadata": {},
   "source": [
    "# SKLEARN comparison\n",
    "\n",
    "The following code using the sklearn library is heavily inspired by the documentation samples: https://scikit-learn.org/stable/modules/tree.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c0637c1-ddd3-4274-a033-8e35989458b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making and training a DecisionTreeClassifier\n",
    "dtclf = sk_tree.DecisionTreeClassifier(criterion=\"gini\",)\n",
    "dtclf = dtclf.fit(X_train, y_train)\n",
    "\n",
    "#sk_tree.plot_tree(clf) #If we want to see the tree\n",
    "\n",
    "#Training my own tree with the best model \n",
    "tree = learn(X_train, y_train, impurity_measure='gini', pruning=True, train_ratio=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40ee4333-bf94-4d5a-84e6-290d3d04ce08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn | train: 1.00000, val: 0.91563, test: 0.89062\n",
      "Mine    | train: 0.91087, val: 0.91875, test: 0.89687\n",
      "\n",
      "Testing prediction functions\n",
      "accuracy_score 0.896875 \n",
      "predict_func 0.896875\n",
      "\n",
      "Results from testing the sklearn tree with cross validation. Mean accuracy: 0.87604\n"
     ]
    }
   ],
   "source": [
    "train_pred, val_pred, test_pred = dtclf.predict(X_train), dtclf.predict(X_val), dtclf.predict(X_test)\n",
    "\n",
    "train_acc_sk = metrics.accuracy_score(y_train, train_pred)\n",
    "val_acc_sk = metrics.accuracy_score(y_val, val_pred) # Would be same as len(val_pred[val_pred == y_val])/len(y_val)\n",
    "test_acc_sk =metrics.accuracy_score(y_test, test_pred)\n",
    "\n",
    "train_acc_my = tree_accuracy(X_train, y_train, tree) \n",
    "val_acc_my = tree_accuracy(X_val, y_val, tree)\n",
    "test_acc_my = tree_accuracy(X_test, y_test, tree)\n",
    "\n",
    "print(f'Sklearn | train: {train_acc_sk:.5f}, val: {val_acc_sk:.5f}, test: {test_acc_sk:.5f}')\n",
    "print(f'Mine    | train: {train_acc_my:.5f}, val: {val_acc_my:.5f}, test: {test_acc_my:.5f}')\n",
    "\n",
    "#Small test to see if predict works like sklearn \n",
    "val_pred = [(predict(x, tree)) for x in X_test]\n",
    "\n",
    "print('\\nTesting prediction functions')\n",
    "print('accuracy_score',metrics.accuracy_score(y_test, val_pred), '\\npredict_func',tree_accuracy(X_test, y_test, tree)) \n",
    "#cv = model_selection.ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "\n",
    "#Testing some cross validation from sklearn library on the \n",
    "cross_val = model_selection.cross_val_score(dtclf, X_train, y_train, cv=5, scoring='f1_macro').mean()\n",
    "\n",
    "print(f'\\nResults from testing the sklearn tree with cross validation. Mean accuracy: {cross_val:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97095d89-5e90-452e-a08a-440f1141c3a9",
   "metadata": {},
   "source": [
    "### Experimenting with some cost pruning\n",
    "https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cceac23e-730e-4c30-abcf-3980a4506398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_pruning(X_train, y_train, X_test, y_test):\n",
    "    clf = sk_tree.DecisionTreeClassifier()\n",
    "    path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "    ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "    clfs = []\n",
    "    for ccp_alpha in ccp_alphas:\n",
    "        clf = sk_tree.DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "        clf.fit(X_train, y_train)\n",
    "        clfs.append(clf)\n",
    "\n",
    "    train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
    "    test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
    "\n",
    "    return train_scores, test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e713f08-993c-4061-a962-65a48298c5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training socre: 0.9593432369038312, best test score: 0.903125\n"
     ]
    }
   ],
   "source": [
    "train_scores, test_scores = sklearn_pruning(X_train, y_train, X_test, y_test)\n",
    "\n",
    "#Finding the model with highest test score. Then its training score\n",
    "train_score, test_score = train_scores[np.nonzero(test_scores == max(test_scores))[0][0]], max(test_scores)\n",
    "\n",
    "print(f'Best training socre: {train_score}, best test score: {test_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1da3090-deb5-43d6-ae54-f5130b46e0d5",
   "metadata": {},
   "source": [
    "## Testing with different dataset\n",
    "\n",
    "The algorithm should be general enough to be used on different datasets. The data has to be clean, and non-categorical however. \n",
    "\n",
    "Lets see how it does with multiple labels, that also are strings\n",
    "\n",
    "found on kaggle: https://www.kaggle.com/datasets/abineshkumark/carsdata\n",
    "\n",
    "The set has some feature that are continious, and a brand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6fcdf727-76be-40df-9dab-c8b735b58b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>cubicinches</th>\n",
       "      <th>hp</th>\n",
       "      <th>weightlbs</th>\n",
       "      <th>time-to-60</th>\n",
       "      <th>year</th>\n",
       "      <th>brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350</td>\n",
       "      <td>165</td>\n",
       "      <td>4209</td>\n",
       "      <td>12</td>\n",
       "      <td>1972</td>\n",
       "      <td>US.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.9</td>\n",
       "      <td>4</td>\n",
       "      <td>89</td>\n",
       "      <td>71</td>\n",
       "      <td>1925</td>\n",
       "      <td>14</td>\n",
       "      <td>1980</td>\n",
       "      <td>Europe.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302</td>\n",
       "      <td>140</td>\n",
       "      <td>3449</td>\n",
       "      <td>11</td>\n",
       "      <td>1971</td>\n",
       "      <td>US.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>400</td>\n",
       "      <td>150</td>\n",
       "      <td>3761</td>\n",
       "      <td>10</td>\n",
       "      <td>1971</td>\n",
       "      <td>US.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.5</td>\n",
       "      <td>4</td>\n",
       "      <td>98</td>\n",
       "      <td>63</td>\n",
       "      <td>2051</td>\n",
       "      <td>17</td>\n",
       "      <td>1978</td>\n",
       "      <td>US.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>305</td>\n",
       "      <td>130</td>\n",
       "      <td>3840</td>\n",
       "      <td>15</td>\n",
       "      <td>1980</td>\n",
       "      <td>US.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>36.1</td>\n",
       "      <td>4</td>\n",
       "      <td>91</td>\n",
       "      <td>60</td>\n",
       "      <td>1800</td>\n",
       "      <td>16</td>\n",
       "      <td>1979</td>\n",
       "      <td>Japan.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>22.0</td>\n",
       "      <td>6</td>\n",
       "      <td>232</td>\n",
       "      <td>112</td>\n",
       "      <td>2835</td>\n",
       "      <td>15</td>\n",
       "      <td>1983</td>\n",
       "      <td>US.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>18.0</td>\n",
       "      <td>6</td>\n",
       "      <td>232</td>\n",
       "      <td>100</td>\n",
       "      <td>3288</td>\n",
       "      <td>16</td>\n",
       "      <td>1972</td>\n",
       "      <td>US.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>22.0</td>\n",
       "      <td>6</td>\n",
       "      <td>250</td>\n",
       "      <td>105</td>\n",
       "      <td>3353</td>\n",
       "      <td>15</td>\n",
       "      <td>1977</td>\n",
       "      <td>US.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mpg   cylinders  cubicinches   hp  weightlbs   time-to-60   year  \\\n",
       "0    14.0           8          350  165       4209           12   1972   \n",
       "1    31.9           4           89   71       1925           14   1980   \n",
       "2    17.0           8          302  140       3449           11   1971   \n",
       "3    15.0           8          400  150       3761           10   1971   \n",
       "4    30.5           4           98   63       2051           17   1978   \n",
       "..    ...         ...          ...  ...        ...          ...    ...   \n",
       "256  17.0           8          305  130       3840           15   1980   \n",
       "257  36.1           4           91   60       1800           16   1979   \n",
       "258  22.0           6          232  112       2835           15   1983   \n",
       "259  18.0           6          232  100       3288           16   1972   \n",
       "260  22.0           6          250  105       3353           15   1977   \n",
       "\n",
       "        brand  \n",
       "0         US.  \n",
       "1     Europe.  \n",
       "2         US.  \n",
       "3         US.  \n",
       "4         US.  \n",
       "..        ...  \n",
       "256       US.  \n",
       "257    Japan.  \n",
       "258       US.  \n",
       "259       US.  \n",
       "260       US.  \n",
       "\n",
       "[261 rows x 8 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cars.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35972c7e-9fa5-4ebb-a0a7-e8ae4b3b49f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the data to numpy\n",
    "data = df.to_numpy()\n",
    "\n",
    "#Getting the labels. They were at the end in this dataset\n",
    "labels = data[:, -1]\n",
    "\n",
    "#Getting everything except the labels\n",
    "data = data[:, :-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e4df006-c6a7-43db-8000-9c44c9e9b533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data was not in perfect condition. It had some broken data, and two columns were in the wrong format.\n",
    "#Cleaning the affected columns.\n",
    "data[data[:, 2] == ' '] = 0 \n",
    "for x in data[:,2]:\n",
    "    x = int(x)\n",
    "\n",
    "data[:, 2] = data[:, 2].astype(int)\n",
    "\n",
    "data[data[:, 4] == ' '] = 0 \n",
    "for x in data[:,4]:\n",
    "    x = int(x)\n",
    "\n",
    "data[:, 4] = data[:, 4].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e8e8026-0046-45e4-9bf0-5dd52941968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into training and test\n",
    "train_data, test_data, train_labels, test_labels = model_selection.train_test_split(data, labels, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9bacae7-2bac-45f1-bd62-8e1855389526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the tree\n",
    "car_tree = learn(train_data, train_labels, impurity_measure='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b65eba8f-1ea0-4a4c-a497-890b8323d0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Europe.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Giving a random data_point to predict\n",
    "predict(test_data[5], car_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7e68722-5f29-448b-8bbb-54d155f2ca1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9951923076923077, 105)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking training accuracy and total nodes\n",
    "tree_accuracy(train_data, train_labels, car_tree), total_nodes(car_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565d2926-d4d1-487f-b56e-dce0a14ce6dd",
   "metadata": {},
   "source": [
    "Seems there are some identical data with different label. Lets see if the sklarn tree gets similar results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68db35fa-17e7-428b-a109-3b520512059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sk_tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "clf = clf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "156c64d8-1e89-4dec-a031-871869635389",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c46bd26-f6a0-4861-9058-95f8a2eb128d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn accuracy is: 0.9951923076923077 with total nodes 65\n"
     ]
    }
   ],
   "source": [
    "print('Sklearn accuracy is:',metrics.accuracy_score(train_labels, predictions), 'with total nodes', clf.tree_.node_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c320a9f-e2f8-4e53-a737-f58b92cbff62",
   "metadata": {},
   "source": [
    "Seems to be similar, except the sklearn tree has fewer nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd1fbcb-ca60-4233-87b7-67f7a20d114e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
